{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3pG2xFhBe7qs",
        "fHfZ9so2fG-c",
        "T0EoilOTt0YR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Tensors\n",
        "You can think of PyTorch tensors as fancy NumPy arrays, designed for use in neural networks. As you can see below, they share many of their basic operations with NumPy."
      ],
      "metadata": {
        "id": "QrnMY317er_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vxVggejcBkf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Think of tensors as NumPy arrays with some NN-friendly bells and whistles.\n",
        "numpy_vector = np.array([1,2,3,4])\n",
        "pytorch_vector = torch.tensor([1,2,3,4])\n",
        "print(numpy_vector)\n",
        "print(pytorch_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can access their shapes the same way.\n",
        "print(numpy_vector.shape)\n",
        "print(pytorch_vector.shape)"
      ],
      "metadata": {
        "id": "lPO9ZwkImW67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating matrices is the same, too.\n",
        "numpy_matrix = np.array([[1,2],[3,4]])\n",
        "pytorch_matrix = torch.tensor([[1,2],[3,4]])\n",
        "print(numpy_matrix)\n",
        "print(pytorch_matrix)"
      ],
      "metadata": {
        "id": "yeq5jpx3oAwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a three dimensional tensor."
      ],
      "metadata": {
        "id": "aAuX6-FLol6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_3d_tensor = np.array([[[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]], [[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]]])\n",
        "pytorch_3d_tensor = torch.tensor([[[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]], [[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]]])\n",
        "print(numpy_3d_tensor)\n",
        "print(pytorch_3d_tensor)"
      ],
      "metadata": {
        "id": "KoQGFRwgogTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the GPU"
      ],
      "metadata": {
        "id": "H88HJxULqLNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "dvWcJGWat3BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "DX-Ff0J3qSwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1,2,3])\n",
        "device_tensor = tensor.to(device)\n",
        "print(f\"{device_tensor} is running on {device_tensor.device}\")"
      ],
      "metadata": {
        "id": "4qXlDIcKwi_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Data Types\n",
        "The complete list of PyTorch data types can be found [here](https://pytorch.org/docs/stable/tensors.html#data-types)."
      ],
      "metadata": {
        "id": "0WopgzTJhtxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "float32 = torch.tensor([1,2], dtype=torch.float32)\n",
        "print(float32)\n",
        "float32.dtype"
      ],
      "metadata": {
        "id": "tqwivMAZiWCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complex64 = torch.tensor([1,2], dtype=torch.complex64)\n",
        "print(complex64)\n",
        "complex64.dtype"
      ],
      "metadata": {
        "id": "EFcnVF_1iqqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int16 = torch.tensor([1,2], dtype=torch.int16)\n",
        "print(int16)\n",
        "int16.dtype"
      ],
      "metadata": {
        "id": "0yq1whXni4_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiplication"
      ],
      "metadata": {
        "id": "kCy0zaake0av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_A = torch.tensor([[1, 2],\n",
        "                         [3, 4],\n",
        "                         [5, 6]])\n",
        "\n",
        "tensor_B = torch.tensor([[7, 10],\n",
        "                         [8, 11],\n",
        "                         [14, 2]])\n",
        "\n",
        "# uncomment the line below and execute. how do you fix it?\n",
        "# torch.matmul(tensor_A, tensor_B) "
      ],
      "metadata": {
        "id": "W_zK77Btl3HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshaping"
      ],
      "metadata": {
        "id": "YVvVbkfgz6Ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensors can also be reshaped with the following methods\n",
        "\n",
        "shapable_tensor = torch.tensor([[1,2,3,4,5], [6,7,8,9,10]])\n",
        "shapable_tensor.reshape(5,2)"
      ],
      "metadata": {
        "id": "id-1LRlyxblF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shapable_tensor.reshape(10,1)"
      ],
      "metadata": {
        "id": "ryN-e_tuuBIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shapable_tensor.unsqueeze(1)"
      ],
      "metadata": {
        "id": "xFVjSltCuJXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shapable_tensor.squeeze(1)"
      ],
      "metadata": {
        "id": "jLOOKOWMudhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(1)"
      ],
      "metadata": {
        "id": "L3GfYUZBxtwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Autograd"
      ],
      "metadata": {
        "id": "3pG2xFhBe7qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember in the last lesson when we said automatic differentiation was the secret sauce of PyTorch? It's commonly called autograd (automatic gradient), and it's described below."
      ],
      "metadata": {
        "id": "7RmyRZv2xWAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a normal tensor, but specify the argument below\n",
        "autograd_tensor = torch.tensor([3.0], requires_grad=True) # defaults to False\n",
        "x = autograd_tensor\n",
        "print(x)"
      ],
      "metadata": {
        "id": "tbjQAt64fTnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd keeps track of the operations performed on this tensor, simplifying the calculation of the derivative when it's time to perform gradient descent.\n",
        "\n",
        "They're sort of like breadcrumbs that PyTorch leaves for itself as the neural net propagates forward, such that it can easily find all the derivatives during backpropagation by following those breadcrumbs back to their source."
      ],
      "metadata": {
        "id": "-a7fgNPf1qMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = x - 5\n",
        "z = y * 3\n",
        "a = (z**2)\n",
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "a.retain_grad()\n",
        "a.backward()\n",
        "\n",
        "print(f\"x is {x}\")\n",
        "print(f\"y is {y}\")\n",
        "print(f\"z is {z}\")\n",
        "print(f\"a is {a}\")\n",
        "print(\"gradient of x is\", x.grad)\n",
        "print(\"gradient of y is\", y.grad)\n",
        "print(\"gradient of z is\", z.grad)"
      ],
      "metadata": {
        "id": "YJ6SJw-71pw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_w = torch.randn(1, requires_grad=True)\n",
        "rand_b = torch.randn(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "AGJjWu-7xrQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Very Simple Example"
      ],
      "metadata": {
        "id": "fHfZ9so2fG-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0])\n",
        "y = torch.tensor([4.0])\n",
        "\n",
        "w = torch.randn(1, what_goes_here?)\n",
        "b = torch.randn(1, what_goes_here?)\n",
        "\n",
        "# update weight via gradient descent\n",
        "for i in range(10):\n",
        "  y_hat = w * x + b\n",
        "  cost = ((y_hat - y)**2)\n",
        "  print(f\"~~for epoch {i+1}~~~\")\n",
        "  print(\"cost:\", round(cost.item(),4))\n",
        "  cost.what_goes_here?\n",
        "  print(w, b, w.grad, b.grad)\n",
        "  with torch.no_grad():\n",
        "    w -= 0.1 * w.grad\n",
        "    b -= 0.1 * b.grad\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "print(f\"w value is {w.item()}, b value is {b.item()}\")"
      ],
      "metadata": {
        "id": "lS1IDRbM5BOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions and Optimizers"
      ],
      "metadata": {
        "id": "T0EoilOTt0YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first import neural net module\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([1.0])\n",
        "y = torch.tensor([4.0])\n",
        ":\n",
        "w = torch.randn(1, what_goes_here?)\n",
        "b = torch.randn(1, what_goes_here?)\n",
        "\n",
        "cost = nn.MSELoss()\n",
        "# update weight via gradient descent\n",
        "for i in range(10):\n",
        "  y_hat = w * x + b\n",
        "  c = cost(y, y_hat)\n",
        "  optimizer = torch.optim.SGD([w, b], lr=0.1)\n",
        "  print(f\"~~for epoch {i+1}~~~\")\n",
        "  print(\"cost:\", round(cost(y, y_hat).item(),4))\n",
        "  c.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "print(f\"w value is {w.item()}, b value is {b.item()}\")"
      ],
      "metadata": {
        "id": "uImyUIpSy0wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}