{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic Neural Net"
      ],
      "metadata": {
        "id": "7vof2v2rWZWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "40dXppo8iI4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XH7Va_H6WY8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HS1Y16wWh8lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_string = \"/content/drive/My Drive/titanic.csv\"\n",
        "data = pd.read_csv(file_string)\n",
        "data"
      ],
      "metadata": {
        "id": "ZFQkoh8bh_Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "m4xkA5_wiFkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "9qHt_ZcIiEK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(axis=0)\n",
        "data"
      ],
      "metadata": {
        "id": "N8m1oxBRVKPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(data)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "kjbKPluvii1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop([\"survived\", \"sex_male\"], axis=1)\n",
        "y = data[\"survived\"]\n",
        "X"
      ],
      "metadata": {
        "id": "asjhPEniil3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Preprocessed Data Into the DataLoader"
      ],
      "metadata": {
        "id": "6YYeHdWZ2_RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "X_tensors = torch.from_numpy(X.astype(float).values).float().to(device)\n",
        "y_tensors = torch.from_numpy(y.astype(float).values).float().to(device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensors, y_tensors, test_size=0.2, train_size=0.8, random_state=42)"
      ],
      "metadata": {
        "id": "zTMGLMHbioen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TitanicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.x_data, self.y_data = X_train, y_train\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.x_data[i], self.y_data[i]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.y_data)\n",
        "\n",
        "titanic_data = TitanicDataset(X_train, y_train)"
      ],
      "metadata": {
        "id": "L0GeWwTL2-Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "loader = DataLoader(dataset=titanic_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "f48dxJhu5j6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "5B3_I-O1jMpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TitanicModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TitanicModel, self).__init__()\n",
        "    # input layer\n",
        "    self.input_layer_to_hidden_layer_1 = nn.Linear(what_goes_here?)\n",
        "    # relu 1\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    # hidden layer 1\n",
        "    self.hidden_layer_1_to_hidden_layer_2 = nn.Linear(what_goes_here?)\n",
        "    # relu 2\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    # hidden layer 2\n",
        "    self.hidden_layer_2_to_output_layer = nn.Linear(what_goes_here?)\n",
        "    # sigmoid 3\n",
        "    self.sigmoid3 = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # input layer\n",
        "    linear_combination1 = self.input_layer_to_hidden_layer_1(what_goes_here?)\n",
        "    intermediate_relu1 = self.relu1(linear_combination1)\n",
        "\n",
        "    # hidden layer 1\n",
        "    linear_combination2 = self.hidden_layer_1_to_hidden_layer_2(intermediate_relu1)\n",
        "    intermediate_relu2 = self.relu2(linear_combination2)\n",
        "\n",
        "    # hidden layer 2\n",
        "    linear_combination3 = self.hidden_layer_2_to_output_layer(intermediate_relu2)\n",
        "    out = self.sigmoid3(what_goes_here?)\n",
        "    return out"
      ],
      "metadata": {
        "id": "VpN2EYUojO57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "4FSxfi9HkXJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = TitanicModel().to(device)\n",
        "epochs = 3000\n",
        "optimizer = torch.optim.Adam(titanic.parameters(), lr = 0.01)\n",
        "loss_function = torch.nn.BCELoss()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "  mini_batch_losses = []\n",
        "  for i, (features, labels) in enumerate(loader):\n",
        "\n",
        "    Y_prediction = titanic.what_goes_here?(features)\n",
        "\n",
        "    loss = loss_function(Y_prediction, labels.unsqueeze(1))\n",
        "\n",
        "    titanic.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # keep track of losses\n",
        "    mini_batch_losses.append(loss.detach().cpu().numpy()) # we can only execute numpy code on the cpu\n",
        "\n",
        "  mini_batch_mean_loss = np.mean(mini_batch_losses)\n",
        "  losses.append(mini_batch_mean_loss)\n",
        "  if epoch % 200 == 0:\n",
        "    print(f\"Loss in Epoch {epoch}: {np.mean(losses)}\")"
      ],
      "metadata": {
        "id": "-y9DahIPkc9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "hLiqzKJ3kqya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "0-RALdbTkxaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_predictions = titanic.forward(X_test)"
      ],
      "metadata": {
        "id": "3nLKpauRk4BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_predictions_thresholded = (y_test_predictions > 0.5) * 1"
      ],
      "metadata": {
        "id": "KklkP5fmk7MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "total_records = len(y_test)\n",
        "num_correct = accuracy_score(y_test.detach().cpu().numpy(), y_test_predictions_thresholded.detach().cpu().numpy(), normalize=False) # normalize would round up to an integer, so we set it False\n",
        "print(num_correct / total_records)"
      ],
      "metadata": {
        "id": "9c_No5MFlAot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}